{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to do 3 rnn \n",
    "1) pure tf \n",
    "2) skflow\n",
    "3) keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10bde8850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEACAYAAABvSbdvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEQI6ZwFAoEgQkjnLBAIBBHi/wOEyQemaLMvjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bde8210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###dataset\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd  \n",
    "from random import random\n",
    "\n",
    "flow = (list(range(1,10,1)) + list(range(10,1,-1)))*1000  \n",
    "pdata = pd.DataFrame({\"a\":flow, \"b\":flow})  \n",
    "pdata.b = pdata.b.shift(9)  \n",
    "data = pdata.iloc[10:] * random()  # some noise  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def _load_data(data, n_prev = 400):  \n",
    "    \"\"\"\n",
    "    data should be pd.DataFrame()\n",
    "    \"\"\"\n",
    "\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-n_prev):\n",
    "        docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
    "        docY.append(data.iloc[i+n_prev].as_matrix())\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "\n",
    "    return alsX, alsY\n",
    "\n",
    "def train_test_split(df, test_size=0.1):  \n",
    "    \"\"\"\n",
    "    This just splits data to training and testing parts\n",
    "    \"\"\"\n",
    "    ntrn = round(len(df) * (1 - test_size))\n",
    "\n",
    "    X_train, y_train = _load_data(df.iloc[0:ntrn])\n",
    "    X_test, y_test = _load_data(df.iloc[ntrn:])\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "ntrn = int(round(len(data) * (1 - 0.1)))\n",
    "X_train, y_train = _load_data(data.iloc[0:ntrn])\n",
    "\n",
    "pd.DataFrame(X_train[1,:,1]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = X_train[1,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "more info when searching for sklearn RNNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/tensorflow/skflow/issues/161"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#regression vars\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(42)\n",
    "data = np.array(list([[2, 1, 2, 2, 3],\n",
    "[2, 2, 3, 4, 5],\n",
    "[3, 3, 1, 2, 1],\n",
    "[2, 4, 5, 4, 1]]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#labels for classification\n",
    "labels = np.array(list([1, 0, 1, 0]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#CLASSIFICATION\n",
    "\n",
    "targets = np.array(list([10, 16, 10, 16]), dtype=np.float32)\n",
    "test_data = np.array(list([[1, 3, 3, 2, 1], [2, 3, 4, 5, 6]]))\n",
    "\n",
    "def input_fn(X):\n",
    "    return tf.split(1, 5, X)\n",
    "\n",
    "    # Classification\n",
    "classifier = skflow.TensorFlowRNNClassifier(rnn_size=2, cell_type='lstm', n_classes=2, input_op_fn=input_fn)\n",
    "classifier.fit(data, labels)\n",
    "classifier.weights_\n",
    "classifier.bias_\n",
    "predictions = classifier.predict(test_data)\n",
    "self.assertAllClose(predictions, np.array([1, 0]))\n",
    "\n",
    "classifier = skflow.TensorFlowRNNClassifier(rnn_size=2, cell_type='rnn', n_classes=2,input_op_fn=input_fn, num_layers=2)\n",
    "classifier.fit(data, labels)\n",
    "classifier = skflow.TensorFlowRNNClassifier(rnn_size=2, cell_type='invalid_cell_type', n_classes=2,input_op_fn=input_fn, num_layers=2)\n",
    "with self.assertRaises(ValueError):\n",
    "    classifier.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11.87805176],\n",
       "       [ 17.21040344]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REGRESSION \n",
    "#Minimal Working example\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tensorflow.contrib import skflow\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def input_fn(X):\n",
    "    return tf.split(1, 1, X)\n",
    "\n",
    "targets = np.array(list([10, 16, 10, 16]), dtype=np.float32)\n",
    "test_data = np.array(list([[1, 3, 3, 2, 1], [2, 3, 4, 5, 6]]))\n",
    "\n",
    "random.seed(42)\n",
    "data = np.array(list([[2, 1, 2, 2, 3],\n",
    "[2, 2, 3, 4, 5],\n",
    "[3, 3, 1, 2, 1],\n",
    "[2, 4, 5, 4, 1]]), dtype=np.float32)\n",
    "\n",
    "regressor = skflow.TensorFlowRNNRegressor(rnn_size=50, cell_type='gru', input_op_fn=input_fn)\n",
    "regressor.fit(data, targets)\n",
    "regressor.weights_\n",
    "regressor.bias_\n",
    "predictions = regressor.predict(np.array(test_data))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the data that is passed to into the RNN is a a vector of all the values for each time step, so if we have 5 time steps \n",
    "\n",
    "  [1,2,3,4,5]\n",
    "  [6,7,8,9,10] \n",
    "  \n",
    "we slice the tensor across the step dimension as \n",
    "[1,6] [2,7] [3,8] etc this is weird to my brain\n",
    "here is the basic tensorflow example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  3.,  3.,  2.,  1.],\n",
       "        [ 2.,  3.,  4.,  5.,  6.]]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  3.,  3.,  2.,  1.],\n",
       "        [ 2.,  3.,  4.,  5.,  6.]]], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 5], name = 'input_data')\n",
    "splitter = tf.split(1,1,X)\n",
    "with tf.Session() as sess:\n",
    "    t = sess.run(splitter, feed_dict= {X:test_data})\n",
    "np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x is generated above \n",
    "# we fit x_T \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from tensorflow.contrib import skflow\n",
    "import tensorflow as tf\n",
    "\n",
    "def input_fn(X):\n",
    "    return tf.split(1, 1, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_training(x, n=len(x), seq_length=20):    \n",
    "    x_t = np.asarray([x[i:i+seq_length] for i in range(n-seq_length)])\n",
    "    y_t = np.asarray([x[i+seq_length] for i in range(n-seq_length)])\n",
    "    return x_t, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_t, y_t = create_training(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 2.35717479,  3.14289972,  3.92862465,  4.71434958,  5.50007451,\n",
      "        6.28579944,  7.07152437,  7.85724931,  7.07152437,  6.28579944,\n",
      "        5.50007451,  4.71434958,  3.92862465,  3.14289972,  2.35717479,\n",
      "        1.57144986,  0.78572493,  1.57144986,  2.35717479,  3.14289972]), 3.9286246525434758)\n"
     ]
    }
   ],
   "source": [
    "print(x_t[0], y_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.10976458]], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = skflow.TensorFlowRNNRegressor(rnn_size=512, cell_type='lstm', input_op_fn=input_fn)\n",
    "regressor.fit(x_t, y_t)\n",
    "regressor.weights_\n",
    "regressor.bias_\n",
    "#predictions = regressor.predict(np.array(test_data))\n",
    "regressor.predict(x_t[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10c416e50>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEACAYAAABvSbdvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzGAwGAGCO2cGg8EIENw5MxgMRoD4/0QdzRaxQPhNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ec1b090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Can we overfit the data? \n",
    "#yes!\n",
    "pd.DataFrame([regressor.predict(x_t[i:i+1])[0][0] for i in range(len(x_t))]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We can then seed the sample with the first 20 from our real data, and let the nerual network\n",
    "#generate new lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated = []\n",
    "seed = x_t[:1,:20]\n",
    "for i in range(200):\n",
    "    seed.shape = (1,20)\n",
    "    y_hat = regressor.predict(seed)[0][0]\n",
    "    new_seed = np.hstack((seed[0],y_hat))\n",
    "    generated.append(y_hat)\n",
    "    seed = new_seed[1:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10cc091d0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEACAYAAABvSbdvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f3cd750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#it works\n",
    "pd.Series(generated).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 20], name = 'input_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "splitter = tf.split(1, 20, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    t = sess.run(splitter, feed_dict= {X:x_t[:1]})\n",
    "    print(t[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "from tensorflow.contrib import skflow\n",
    "\n",
    "\n",
    "#Notes \n",
    "#The number of lstm units is 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Source for the TensorFlowRNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TensorFlowRNNRegressor(TensorFlowEstimator, RegressorMixin):\n",
    "    \"\"\"TensorFlow RNN Regressor model.\n",
    "    Parameters:\n",
    "        rnn_size: The size for rnn cell, e.g. size of your word embeddings.\n",
    "        cell_type: The type of rnn cell, including rnn, gru, and lstm.\n",
    "        num_layers: The number of layers of the rnn model.\n",
    "        input_op_fn: Function that will transform the input tensor, such as\n",
    "                     creating word embeddings, byte list, etc. This takes\n",
    "                     an argument X for input and returns transformed X.\n",
    "        bidirectional: boolean, Whether this is a bidirectional rnn.\n",
    "        sequence_length: If sequence_length is provided, dynamic calculation is performed.\n",
    "                 This saves computational time when unrolling past max sequence length.\n",
    "        initial_state: An initial state for the RNN. This must be a tensor of appropriate type\n",
    "                       and shape [batch_size x cell.state_size].\n",
    "        tf_master: TensorFlow master. Empty string is default for local.\n",
    "        batch_size: Mini batch size.\n",
    "        steps: Number of steps to run over data.\n",
    "        optimizer: Optimizer name (or class), for example \"SGD\", \"Adam\",\n",
    "                   \"Adagrad\".\n",
    "        learning_rate: If this is constant float value, no decay function is used.\n",
    "            Instead, a customized decay function can be passed that accepts\n",
    "            global_step as parameter and returns a Tensor.\n",
    "            e.g. exponential decay function:\n",
    "            def exp_decay(global_step):\n",
    "                return tf.train.exponential_decay(\n",
    "                    learning_rate=0.1, global_step,\n",
    "                    decay_steps=2, decay_rate=0.001)\n",
    "        tf_random_seed: Random seed for TensorFlow initializers.\n",
    "            Setting this value, allows consistency between reruns.\n",
    "        continue_training: when continue_training is True, once initialized\n",
    "            model will be continuely trained on every call of fit.\n",
    "        num_cores: Number of cores to be used. (default: 4)\n",
    "        verbose: Controls the verbosity, possible values:\n",
    "                 0: the algorithm and debug information is muted.\n",
    "                 1: trainer prints the progress.\n",
    "                 2: log device placement is printed.\n",
    "        max_to_keep: The maximum number of recent checkpoint files to keep.\n",
    "            As new files are created, older files are deleted.\n",
    "            If None or 0, all checkpoint files are kept.\n",
    "            Defaults to 5 (that is, the 5 most recent checkpoint files are kept.)\n",
    "        keep_checkpoint_every_n_hours: Number of hours between each checkpoint\n",
    "            to be saved. The default value of 10,000 hours effectively disables the feature.\n",
    "   \"\"\"\n",
    "\n",
    "    def __init__(self, rnn_size, cell_type='gru', num_layers=1,\n",
    "                 input_op_fn=null_input_op_fn, initial_state=None,\n",
    "                 bidirectional=False, sequence_length=None,\n",
    "                 n_classes=0, tf_master=\"\", batch_size=32,\n",
    "                 steps=50, optimizer=\"SGD\", learning_rate=0.1,\n",
    "                 tf_random_seed=42, continue_training=False,\n",
    "                 config_addon=None, verbose=1,\n",
    "                 max_to_keep=5, keep_checkpoint_every_n_hours=10000):\n",
    "\n",
    "        self.rnn_size = rnn_size\n",
    "        self.cell_type = cell_type\n",
    "        self.input_op_fn = input_op_fn\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.sequence_length = sequence_length\n",
    "        self.initial_state = initial_state\n",
    "        super(TensorFlowRNNRegressor, self).__init__(\n",
    "            model_fn=self._model_fn,\n",
    "            n_classes=n_classes, tf_master=tf_master,\n",
    "            batch_size=batch_size, steps=steps, optimizer=optimizer,\n",
    "            learning_rate=learning_rate, tf_random_seed=tf_random_seed,\n",
    "            continue_training=continue_training, config_addon=config_addon,\n",
    "            verbose=verbose, max_to_keep=max_to_keep,\n",
    "            keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n",
    "\n",
    "    def _model_fn(self, X, y):\n",
    "        return models.get_rnn_model(self.rnn_size, self.cell_type,\n",
    "                                    self.num_layers,\n",
    "                                    self.input_op_fn, self.bidirectional,\n",
    "                                    models.linear_regression,\n",
    "                                    self.sequence_length,\n",
    "                                    self.initial_state)(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_model(rnn_size, cell_type, num_layers, input_op_fn,\n",
    "                  bidirectional, target_predictor_fn,\n",
    "                  sequence_length, initial_state):\n",
    "    \"\"\"Returns a function that creates a RNN TensorFlow subgraph with given\n",
    "    params.\n",
    "    Args:\n",
    "        rnn_size: The size for rnn cell, e.g. size of your word embeddings.\n",
    "        cell_type: The type of rnn cell, including rnn, gru, and lstm.\n",
    "        num_layers: The number of layers of the rnn model.\n",
    "        input_op_fn: Function that will transform the input tensor, such as\n",
    "                     creating word embeddings, byte list, etc. This takes\n",
    "                     an argument X for input and returns transformed X.\n",
    "        bidirectional: boolean, Whether this is a bidirectional rnn.\n",
    "        target_predictor_fn: Function that will predict target from input\n",
    "                             features. This can be logistic regression,\n",
    "                             linear regression or any other model,\n",
    "                             that takes X, y and returns predictions and loss tensors.\n",
    "        sequence_length: If sequence_length is provided, dynamic calculation is performed.\n",
    "                         This saves computational time when unrolling past max sequence length.\n",
    "                         Required for bidirectional RNNs.\n",
    "        initial_state: An initial state for the RNN. This must be a tensor of appropriate type\n",
    "                       and shape [batch_size x cell.state_size].\n",
    "    Returns:\n",
    "        A function that creates the subgraph.\n",
    "    \"\"\"\n",
    "    def rnn_estimator(X, y):\n",
    "        \"\"\"RNN estimator with target predictor function on top.\"\"\"\n",
    "        X = input_op_fn(X)\n",
    "        if cell_type == 'rnn':\n",
    "            cell_fn = tf.nn.rnn_cell.BasicRNNCell\n",
    "        elif cell_type == 'gru':\n",
    "            cell_fn = tf.nn.rnn_cell.GRUCell\n",
    "        elif cell_type == 'lstm':\n",
    "            cell_fn = tf.nn.rnn_cell.BasicLSTMCell\n",
    "        else:\n",
    "            raise ValueError(\"cell_type {} is not supported. \".format(cell_type))\n",
    "        if bidirectional:\n",
    "            # forward direction cell\n",
    "            rnn_fw_cell = tf.nn.rnn_cell.MultiRNNCell([cell_fn(rnn_size)] * num_layers)\n",
    "            # backward direction cell\n",
    "            rnn_bw_cell = tf.nn.rnn_cell.MultiRNNCell([cell_fn(rnn_size)] * num_layers)\n",
    "            # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n",
    "            _, encoding = bidirectional_rnn(rnn_fw_cell, rnn_bw_cell, X,\n",
    "                                            dtype=tf.float32,\n",
    "                                            sequence_length=sequence_length,\n",
    "                                            initial_state_fw=initial_state,\n",
    "                                            initial_state_bw=initial_state)\n",
    "        else:\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([cell_fn(rnn_size)] * num_layers)\n",
    "            _, encoding = tf.nn.rnn(cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=sequence_length,\n",
    "                                    initial_state=initial_state)\n",
    "        return target_predictor_fn(encoding, y)\n",
    "    return rnn_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#train = pandas.read_csv('dbpedia_csv/train.csv', header=None)\n",
    "#X_train, y_train = train[2], train[0]\n",
    "#test = pandas.read_csv('dbpedia_csv/test.csv', header=None)\n",
    "#X_test, y_test = test[2], test[0]\n",
    "\n",
    "### Process vocabulary\n",
    "\n",
    "#MAX_DOCUMENT_LENGTH = 10\n",
    "\n",
    "#vocab_processor = skflow.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "#X_train = np.array(list(vocab_processor.fit_transform(X_train)))\n",
    "#X_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "#n_words = len(vocab_processor.vocabulary_)\n",
    "#print('Total words: %d' % n_words)\n",
    "\n",
    "### Models\n",
    "\n",
    "#EMBEDDING_SIZE = 50\n",
    "\n",
    "# Customized function to transform batched X into embeddings\n",
    "\n",
    "input_op_fn = lambda x:x\n",
    "\n",
    "\n",
    "# Single direction GRU with a single layer\n",
    "classifier = skflow.TensorFlowRNNRegressor(rnn_size=1, \n",
    "     cell_type='gru',  batch_size = 1, \n",
    "    num_layers=1, bidirectional=False, sequence_length=None,\n",
    "    steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)\n",
    "\n",
    "# Continously train for 1000 steps & predict on test set.\n",
    "#while True:\n",
    "#    classifier.fit(X_train, y_train, logdir='/tmp/tf_examples/word_rnn')\n",
    "#    score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "#    print('Accuracy: {0:f}'.format(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 743040\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pandas.read_csv('dbpedia_csv/train.csv', header=None)\n",
    "X_train, y_train = train[2], train[0]\n",
    "test = pandas.read_csv('dbpedia_csv/test.csv', header=None)\n",
    "X_test, y_test = test[2], test[0]\n",
    "\n",
    "### Process vocabulary\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "\n",
    "\n",
    "vocab_processor = skflow.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "X_train = np.array(list(vocab_processor.fit_transform(X_train)))\n",
    "X_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_op_fn(X):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = skflow.ops.categorical_variable(X, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = skflow.ops.split_squeeze(1, MAX_DOCUMENT_LENGTH, word_vectors)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 50\n",
    "t = input_op_fn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 743040\n"
     ]
    }
   ],
   "source": [
    "train = pandas.read_csv('dbpedia_csv/train.csv', header=None)\n",
    "X_train, y_train = train[2], train[0]\n",
    "test = pandas.read_csv('dbpedia_csv/test.csv', header=None)\n",
    "X_test, y_test = test[2], test[0]\n",
    "\n",
    "### Process vocabulary\n",
    "\n",
    "MAX_DOCUMENT_LENGTH = 10\n",
    "\n",
    "\n",
    "vocab_processor = skflow.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "X_train = np.array(list(vocab_processor.fit_transform(X_train)))\n",
    "X_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)\n",
    "\n",
    "### Models\n",
    "\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "# Customized function to transform batched X into embeddings\n",
    "def input_op_fn(X):\n",
    "    # Convert indexes of words into embeddings.\n",
    "    # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "    # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "    # EMBEDDING_SIZE].\n",
    "    word_vectors = skflow.ops.categorical_variable(X, n_classes=n_words,\n",
    "        embedding_size=EMBEDDING_SIZE, name='words')\n",
    "    # Split into list of embedding per word, while removing doc length dim.\n",
    "    # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "    word_list = skflow.ops.split_squeeze(1, MAX_DOCUMENT_LENGTH, word_vectors)\n",
    "    return word_list\n",
    "\n",
    "# Single direction GRU with a single layer\n",
    "classifier = skflow.TensorFlowRNNClassifier(rnn_size=EMBEDDING_SIZE, \n",
    "    n_classes=15, cell_type='gru', input_op_fn=input_op_fn,\n",
    "    num_layers=1, bidirectional=False, sequence_length=None,\n",
    "    steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)\n",
    "\n",
    "# Continously train for 1000 steps & predict on test set.\n",
    "#while True:\n",
    "#    classifier.fit(X_train, y_train, logdir='/tmp/tf_examples/word_rnn')\n",
    "#    score = metrics.accuracy_score(y_test, classifier.predict(X_test))\n",
    "#    print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-f901c04037b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, monitor, logdir)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0msummary_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0msummaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summaries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             feed_params_fn=self._data_feeder.get_feed_params)\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/trainer.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(session, train_op, loss, global_step, feed_dict_fn, steps, monitor, summary_writer, summaries, feed_params_fn)\u001b[0m\n\u001b[1;32m     51\u001b[0m             global_step_value, loss_value, summ, _ = session.run(\n\u001b[1;32m     52\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             global_step_value, loss_value, _ = session.run(\n",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 564\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 637\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ryanwheeler/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m--> 628\u001b[0;31m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"LSTM for time series classification\n",
    "Made: 30 march 2016\n",
    "This model takes in time series and class labels.\n",
    "The LSTM models the time series. A fully-connected layer\n",
    "generates an output to be classified with Softmax\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "\n",
    "\n",
    "def sample_batch(X_train,y_train,batch_size,num_steps):\n",
    "    \"\"\" Function to sample a batch for training\"\"\"\n",
    "    N,data_len = X_train.shape\n",
    "    ind_N = np.random.choice(N,batch_size,replace=False)\n",
    "    ind_start = np.random.choice(data_len-num_steps,1)\n",
    "    #form batch\n",
    "    X_batch = X_train[ind_N,ind_start:ind_start+num_steps]\n",
    "    y_batch = y_train[ind_N]\n",
    "    return X_batch,y_batch\n",
    "    \n",
    "def check_test(X_test,y_test,batch_size,num_steps):\n",
    "    \"\"\" Function to check the test_accuracy on the entire test set\n",
    "    This is a workaround. I haven't figured out yet how to make the graph\n",
    "    general for multiple batch sizes.\"\"\"\n",
    "    N = X_test.shape[0]\n",
    "    num_batch = np.floor(N/batch_size)\n",
    "    test_acc = np.zeros(num_batch)\n",
    "    for i in range(int(num_batch)):\n",
    "      X_batch, y_batch = sample_batch(X_test,y_test,batch_size,num_steps)\n",
    "      test_acc[i] = session.run(accuracy,feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "    return np.mean(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Hyperparamaters\"\"\"\n",
    "init_scale = 0.08           #Initial scale for the states\n",
    "max_grad_norm = 25          #Clipping of the gradient before update\n",
    "num_layers = 2              #Number of stacked LSTM layers\n",
    "num_steps = 32              #Number of steps to backprop over at every batch\n",
    "hidden_size = 13            #Number of entries of the cell state of the LSTM\n",
    "max_iterations = 2000       #Maximum iterations to train\n",
    "batch_size = 30             #Batch size\n",
    "\n",
    "\n",
    "\"\"\"Place holders\"\"\"\n",
    "input_data = tf.placeholder(tf.float32, [None, num_steps], name = 'input_data')\n",
    "targets = tf.placeholder(tf.int64, [None], name='Targets')\n",
    "#Used later on for drop_out. At testtime, we pass 1.0\n",
    "keep_prob = tf.placeholder(\"float\", name = 'Drop_out_keep_prob')  \n",
    "\n",
    "initializer = tf.random_uniform_initializer(-init_scale,init_scale)\n",
    "with tf.variable_scope(\"model\", initializer=initializer):\n",
    "  \"\"\"Define the basis LSTM\"\"\"\n",
    "  with tf.name_scope(\"LSTM_setup\") as scope:\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    if keep_prob < 1:\n",
    "      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)   #Initialize the zero_state. Note that it has to be run in session-time\n",
    "    #We have only one input dimension, but we generalize our code for future expansion\n",
    "    inputs = tf.expand_dims(input_data, 2)\n",
    "\n",
    "  #Define the recurrent nature of the LSTM\n",
    "  #Re-use variables only after first time-step\n",
    "  with tf.name_scope(\"LSTM\") as scope:\n",
    "    outputs = []\n",
    "    state = initial_state\n",
    "    with tf.variable_scope(\"LSTM_state\"):\n",
    "      for time_step in range(num_steps):\n",
    "       if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "       (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "       outputs.append(cell_output)       #Now cell_output is size [batch_size x hidden_size]\n",
    "\n",
    "\n",
    "#Generate a classification from the last cell_output\n",
    "#Note, this is where timeseries classification differs from sequence to sequence\n",
    "#modelling. We only output to Softmax at last time step\n",
    "with tf.name_scope(\"Softmax\") as scope:\n",
    "  with tf.variable_scope(\"Softmax_params\"): \n",
    "    # Both datasets have four output classes. Improve the code by changing the 4\n",
    "    # into a hyperparameter\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, 4])                         \n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [4])                               \n",
    "  logits = tf.matmul(cell_output, softmax_w) + softmax_b\n",
    "  #Use sparse Softmax because we have mutually exclusive classes    \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,targets,name = 'Sparse_softmax')\n",
    "  cost = tf.reduce_sum(loss) / batch_size\n",
    "  #Pass on a summary to Tensorboard\n",
    "  cost_summ = tf.scalar_summary('Cost',cost)\n",
    "  # Calculate the accuracy\n",
    "  correct_prediction = tf.equal(tf.argmax(logits,1), targets)\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "  accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\"\"\"Optimizer\"\"\"\n",
    "with tf.name_scope(\"Optimizer\") as scope:\n",
    "  tvars = tf.trainable_variables()\n",
    "  #We clip the gradients to prevent explosion\n",
    "  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),max_grad_norm)\n",
    "  optimizer = tf.train.AdamOptimizer(8e-3)\n",
    "  gradients = zip(grads, tvars)\n",
    "  train_op = optimizer.apply_gradients(gradients)\n",
    "  # Add histograms for variables, gradients and gradient norms.\n",
    "  # The for-loop loops over all entries of the gradient and plots\n",
    "  # a histogram. We cut of\n",
    "  for gradient, variable in gradients:\n",
    "    if isinstance(gradient, ops.IndexedSlices):\n",
    "      grad_values = gradient.values\n",
    "    else:\n",
    "      grad_values = gradient\n",
    "    h1 = tf.histogram_summary(variable.name, variable)\n",
    "    h2 = tf.histogram_summary(variable.name + \"/gradients\", grad_values)\n",
    "    h3 = tf.histogram_summary(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "  \n",
    "\"\"\"Load the data\"\"\"\n",
    "dummy = True\n",
    "if dummy:\n",
    "  data_train = np.loadtxt('UCR_TS_Archive_2015/Two_Patterns/Two_Patterns_TRAIN',delimiter=',')\n",
    "  data_test_val = np.loadtxt('UCR_TS_Archive_2015/Two_Patterns/Two_Patterns_TEST',delimiter=',')\n",
    "else:\n",
    "  data_train = np.loadtxt('data_train_dummy',delimiter=',')\n",
    "  data_test_val = np.loadtxt('data_test_dummy',delimiter=',')\n",
    "data_test,data_val = np.split(data_test_val,2)\n",
    "X_train = data_train[:,1:]\n",
    "X_val = data_val[:,1:]\n",
    "X_test = data_test[:,1:]\n",
    "N = X_train.shape[0]\n",
    "Ntest = X_test.shape[0]\n",
    "# Targets have labels 1-indexed. We subtract one for 0-indexed\n",
    "y_train = data_train[:,0]-1\n",
    "y_val = data_val[:,0]-1\n",
    "y_test = data_test[:,0]-1\n",
    "\n",
    "#Final code for the TensorBoard\n",
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "# Collect the costs in a numpy fashion\n",
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train with approximately %d epochs' %(epochs))\n",
    "perf_collect = np.zeros((3,int(np.floor(max_iterations /100))))\n",
    "\n",
    "\"\"\"Session time\"\"\"\n",
    "with tf.Session() as session:\n",
    "  writer = tf.train.SummaryWriter(\"/home/rob/Dropbox/ml_projects/LSTM/log_tb\", session.graph_def)\n",
    "  tf.initialize_all_variables().run()\n",
    "  \n",
    "  \n",
    "  step = 0\n",
    "  for i in range(max_iterations):\n",
    "    \n",
    "    # Calculate some sizes\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    #Sample batch for training\n",
    "    X_batch, y_batch = sample_batch(X_train,y_train,batch_size,num_steps)\n",
    "    state = initial_state.eval()  #Fire up the LSTM\n",
    "    \n",
    "    #Next line does the actual training\n",
    "    session.run(train_op,feed_dict = {input_data: X_batch,targets: y_batch,initial_state: state,keep_prob:0.5})\n",
    "    if i==0:\n",
    "        # Uset this line to check before-and-after test accuracy\n",
    "        acc_test_before = check_test(X_test,y_test,batch_size,num_steps)\n",
    "    if i%100 == 0:\n",
    "      #Evaluate training performance\n",
    "      X_batch, y_batch = sample_batch(X_train,y_train,batch_size,num_steps)                           \n",
    "      cost_out = session.run(cost,feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "      perf_collect[0,step] = cost_out\n",
    "      #print('At %d out of %d train cost is %.3f' %(i,max_iterations,cost_out)) #Uncomment line to follow train cost\n",
    "\n",
    "      #Evaluate validation performance\n",
    "      X_batch, y_batch = sample_batch(X_val,y_val,batch_size,num_steps)\n",
    "      result = session.run([cost,merged,accuracy],feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "      cost_out = result[0]\n",
    "      perf_collect[1,step] = cost_out\n",
    "      acc_val = result[2]\n",
    "      perf_collect[2,step] = acc_val\n",
    "      print('At %d out of %d val cost is %.3f and val acc is %.3f' %(i,max_iterations,cost_out,acc_val))\n",
    "      \n",
    "      #Write information to TensorBoard\n",
    "      summary_str = result[1]\n",
    "      writer.add_summary(summary_str, i)\n",
    "      writer.flush()\n",
    "      \n",
    "      step +=1\n",
    "  acc_test = check_test(X_test,y_test,batch_size,num_steps)\n",
    "\n",
    "\"\"\"Additional plots\"\"\"\n",
    "print('The accuracy on the test data is %.3f, before training was %.3f' %(acc_test,acc_test_before))\n",
    "plt.plot(perf_collect[0],label='Train')\n",
    "plt.plot(perf_collect[1],label = 'Valid')\n",
    "plt.plot(perf_collect[2],label = 'Valid accuracy')\n",
    "plt.axis([0, step, 0, np.max(perf_collect)])\n",
    "plt.legend()\n",
    "plt.show(\"\"\"LSTM for time series classification\n",
    "Made: 30 march 2016\n",
    "This model takes in time series and class labels.\n",
    "The LSTM models the time series. A fully-connected layer\n",
    "generates an output to be classified with Softmax\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import clip_ops\n",
    "\n",
    "\n",
    "def sample_batch(X_train,y_train,batch_size,num_steps):\n",
    "    \"\"\" Function to sample a batch for training\"\"\"\n",
    "    N,data_len = X_train.shape\n",
    "    ind_N = np.random.choice(N,batch_size,replace=False)\n",
    "    ind_start = np.random.choice(data_len-num_steps,1)\n",
    "    #form batch\n",
    "    X_batch = X_train[ind_N,ind_start:ind_start+num_steps]\n",
    "    y_batch = y_train[ind_N]\n",
    "    return X_batch,y_batch\n",
    "    \n",
    "def check_test(X_test,y_test,batch_size,num_steps):\n",
    "    \"\"\" Function to check the test_accuracy on the entire test set\n",
    "    This is a workaround. I haven't figured out yet how to make the graph\n",
    "    general for multiple batch sizes.\"\"\"\n",
    "    N = X_test.shape[0]\n",
    "    num_batch = np.floor(N/batch_size)\n",
    "    test_acc = np.zeros(num_batch)\n",
    "    for i in range(int(num_batch)):\n",
    "      X_batch, y_batch = sample_batch(X_test,y_test,batch_size,num_steps)\n",
    "      test_acc[i] = session.run(accuracy,feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "    return np.mean(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Hyperparamaters\"\"\"\n",
    "init_scale = 0.08           #Initial scale for the states\n",
    "max_grad_norm = 25          #Clipping of the gradient before update\n",
    "num_layers = 2              #Number of stacked LSTM layers\n",
    "num_steps = 32              #Number of steps to backprop over at every batch\n",
    "hidden_size = 13            #Number of entries of the cell state of the LSTM\n",
    "max_iterations = 2000       #Maximum iterations to train\n",
    "batch_size = 30             #Batch size\n",
    "\n",
    "\n",
    "\"\"\"Place holders\"\"\"\n",
    "input_data = tf.placeholder(tf.float32, [None, num_steps], name = 'input_data')\n",
    "targets = tf.placeholder(tf.int64, [None], name='Targets')\n",
    "#Used later on for drop_out. At testtime, we pass 1.0\n",
    "keep_prob = tf.placeholder(\"float\", name = 'Drop_out_keep_prob')  \n",
    "\n",
    "initializer = tf.random_uniform_initializer(-init_scale,init_scale)\n",
    "with tf.variable_scope(\"model\", initializer=initializer):\n",
    "  \"\"\"Define the basis LSTM\"\"\"\n",
    "  with tf.name_scope(\"LSTM_setup\") as scope:\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    if keep_prob < 1:\n",
    "      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "      cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)   #Initialize the zero_state. Note that it has to be run in session-time\n",
    "    #We have only one input dimension, but we generalize our code for future expansion\n",
    "    inputs = tf.expand_dims(input_data, 2)\n",
    "\n",
    "  #Define the recurrent nature of the LSTM\n",
    "  #Re-use variables only after first time-step\n",
    "  with tf.name_scope(\"LSTM\") as scope:\n",
    "    outputs = []\n",
    "    state = initial_state\n",
    "    with tf.variable_scope(\"LSTM_state\"):\n",
    "      for time_step in range(num_steps):\n",
    "       if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "       (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "       outputs.append(cell_output)       #Now cell_output is size [batch_size x hidden_size]\n",
    "\n",
    "\n",
    "#Generate a classification from the last cell_output\n",
    "#Note, this is where timeseries classification differs from sequence to sequence\n",
    "#modelling. We only output to Softmax at last time step\n",
    "with tf.name_scope(\"Softmax\") as scope:\n",
    "  with tf.variable_scope(\"Softmax_params\"): \n",
    "    # Both datasets have four output classes. Improve the code by changing the 4\n",
    "    # into a hyperparameter\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [hidden_size, 4])                         \n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [4])                               \n",
    "  logits = tf.matmul(cell_output, softmax_w) + softmax_b\n",
    "  #Use sparse Softmax because we have mutually exclusive classes    \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,targets,name = 'Sparse_softmax')\n",
    "  cost = tf.reduce_sum(loss) / batch_size\n",
    "  #Pass on a summary to Tensorboard\n",
    "  cost_summ = tf.scalar_summary('Cost',cost)\n",
    "  # Calculate the accuracy\n",
    "  correct_prediction = tf.equal(tf.argmax(logits,1), targets)\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "  accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "\"\"\"Optimizer\"\"\"\n",
    "with tf.name_scope(\"Optimizer\") as scope:\n",
    "  tvars = tf.trainable_variables()\n",
    "  #We clip the gradients to prevent explosion\n",
    "  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),max_grad_norm)\n",
    "  optimizer = tf.train.AdamOptimizer(8e-3)\n",
    "  gradients = zip(grads, tvars)\n",
    "  train_op = optimizer.apply_gradients(gradients)\n",
    "  # Add histograms for variables, gradients and gradient norms.\n",
    "  # The for-loop loops over all entries of the gradient and plots\n",
    "  # a histogram. We cut of\n",
    "  for gradient, variable in gradients:\n",
    "    if isinstance(gradient, ops.IndexedSlices):\n",
    "      grad_values = gradient.values\n",
    "    else:\n",
    "      grad_values = gradient\n",
    "    h1 = tf.histogram_summary(variable.name, variable)\n",
    "    h2 = tf.histogram_summary(variable.name + \"/gradients\", grad_values)\n",
    "    h3 = tf.histogram_summary(variable.name + \"/gradient_norm\", clip_ops.global_norm([grad_values]))\n",
    "  \n",
    "\"\"\"Load the data\"\"\"\n",
    "dummy = True\n",
    "if dummy:\n",
    "  data_train = np.loadtxt('UCR_TS_Archive_2015/Two_Patterns/Two_Patterns_TRAIN',delimiter=',')\n",
    "  data_test_val = np.loadtxt('UCR_TS_Archive_2015/Two_Patterns/Two_Patterns_TEST',delimiter=',')\n",
    "else:\n",
    "  data_train = np.loadtxt('data_train_dummy',delimiter=',')\n",
    "  data_test_val = np.loadtxt('data_test_dummy',delimiter=',')\n",
    "data_test,data_val = np.split(data_test_val,2)\n",
    "X_train = data_train[:,1:]\n",
    "X_val = data_val[:,1:]\n",
    "X_test = data_test[:,1:]\n",
    "N = X_train.shape[0]\n",
    "Ntest = X_test.shape[0]\n",
    "# Targets have labels 1-indexed. We subtract one for 0-indexed\n",
    "y_train = data_train[:,0]-1\n",
    "y_val = data_val[:,0]-1\n",
    "y_test = data_test[:,0]-1\n",
    "\n",
    "#Final code for the TensorBoard\n",
    "merged = tf.merge_all_summaries()\n",
    "\n",
    "\n",
    "# Collect the costs in a numpy fashion\n",
    "epochs = np.floor(batch_size*max_iterations / N)\n",
    "print('Train with approximately %d epochs' %(epochs))\n",
    "perf_collect = np.zeros((3,int(np.floor(max_iterations /100))))\n",
    "\n",
    "\"\"\"Session time\"\"\"\n",
    "with tf.Session() as session:\n",
    "  writer = tf.train.SummaryWriter(\"/home/rob/Dropbox/ml_projects/LSTM/log_tb\", session.graph_def)\n",
    "  tf.initialize_all_variables().run()\n",
    "  \n",
    "  \n",
    "  step = 0\n",
    "  for i in range(max_iterations):\n",
    "    \n",
    "    # Calculate some sizes\n",
    "    N = X_train.shape[0]\n",
    "    \n",
    "    #Sample batch for training\n",
    "    X_batch, y_batch = sample_batch(X_train,y_train,batch_size,num_steps)\n",
    "    state = initial_state.eval()  #Fire up the LSTM\n",
    "    \n",
    "    #Next line does the actual training\n",
    "    session.run(train_op,feed_dict = {input_data: X_batch,targets: y_batch,initial_state: state,keep_prob:0.5})\n",
    "    if i==0:\n",
    "        # Uset this line to check before-and-after test accuracy\n",
    "        acc_test_before = check_test(X_test,y_test,batch_size,num_steps)\n",
    "    if i%100 == 0:\n",
    "      #Evaluate training performance\n",
    "      X_batch, y_batch = sample_batch(X_train,y_train,batch_size,num_steps)                           \n",
    "      cost_out = session.run(cost,feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "      perf_collect[0,step] = cost_out\n",
    "      #print('At %d out of %d train cost is %.3f' %(i,max_iterations,cost_out)) #Uncomment line to follow train cost\n",
    "\n",
    "      #Evaluate validation performance\n",
    "      X_batch, y_batch = sample_batch(X_val,y_val,batch_size,num_steps)\n",
    "      result = session.run([cost,merged,accuracy],feed_dict = {input_data: X_batch, targets: y_batch, initial_state:state,keep_prob:1})\n",
    "      cost_out = result[0]\n",
    "      perf_collect[1,step] = cost_out\n",
    "      acc_val = result[2]\n",
    "      perf_collect[2,step] = acc_val\n",
    "      print('At %d out of %d val cost is %.3f and val acc is %.3f' %(i,max_iterations,cost_out,acc_val))\n",
    "      \n",
    "      #Write information to TensorBoard\n",
    "      summary_str = result[1]\n",
    "      writer.add_summary(summary_str, i)\n",
    "      writer.flush()\n",
    "      \n",
    "      step +=1\n",
    "  acc_test = check_test(X_test,y_test,batch_size,num_steps)\n",
    "\n",
    "\"\"\"Additional plots\"\"\"\n",
    "print('The accuracy on the test data is %.3f, before training was %.3f' %(acc_test,acc_test_before))\n",
    "plt.plot(perf_collect[0],label='Train')\n",
    "plt.plot(perf_collect[1],label = 'Valid')\n",
    "plt.plot(perf_collect[2],label = 'Valid accuracy')\n",
    "plt.axis([0, step, 0, np.max(perf_collect)])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
